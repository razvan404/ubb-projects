{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGpOGQHgVoKM"
      },
      "source": [
        "# Laboratory 2\n",
        "\n",
        "In this second laboratory you will implement and train a simple softmax classifier for images.\n",
        "Another important aspect that we'll be discussing today is how to properly evaluate a classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0--EpYvTEM3"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "%cd gdrive/My Drive"
      ],
      "metadata": {
        "id": "ZUUiXsm48GNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "![ -d cvdl2_lab2 ] || mkdir cvdl2_lab2\n",
        "%cd cvdl2_lab2"
      ],
      "metadata": {
        "id": "fuD0XaSzt0AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6YAQldx8ZLt"
      },
      "source": [
        "![ -d cvdl_lab2_students ] || git clone https://github.com/dianalauraborza/cvdl_lab2_students.git\n",
        "\n",
        "# allow \"hot-reloading\" of modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# needed for inline plots in some contexts\n",
        "%matplotlib inline\n",
        "%cd cvdl_lab2_students"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.view('lab2/activations.py')"
      ],
      "metadata": {
        "id": "LY1gLFjj_uAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxfMZRro1rM9"
      },
      "source": [
        "## Warm-up. numpy exercise\n",
        "\n",
        "Let's start by implementing the softmax function. This function takes as input an array of *N* arbitrary numbers and normalizes the array such that the output is a probability distribution.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "softmax(x)_i = \\frac{e^{x_i}}{\\sum_{j = 0}^{j = N} e^{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "In the file *activations.py* write the implementation of the softmax function.\n",
        "\n",
        "**Short discussion about softmax numerical stability.** You can also check this [post](https://ogunlao.github.io/2020/04/26/you_dont_really_know_softmax.html).\n",
        "\n",
        "This code is found in _activations.py_ ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_My code_:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x, t=1):\n",
        "    \"\"\"\"\n",
        "    Applies the softmax temperature on the input x, using the temperature t\n",
        "    \"\"\"\n",
        "    # TODO your code here\n",
        "    x /= t\n",
        "    x = np.exp(x)\n",
        "    x = x / np.sum(x, axis=0)\n",
        "    # end TODO your code here\n",
        "    return x\n",
        "```"
      ],
      "metadata": {
        "id": "a3KCBAyFawrm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opIOjvif1g0G"
      },
      "source": [
        "from lab2.activations import softmax\n",
        "\n",
        "# validate softmax\n",
        "# let's check that you obtained the same values\n",
        "# as the softmax implementation in torch\n",
        "arr = np.asarray([2, 4, 10, 100, 2.0])\n",
        "assert (np.allclose(torch.nn.functional.softmax(torch.from_numpy(arr), dim=0).numpy(), softmax(arr)))\n",
        "arr = np.asarray([0.0, 0, 0, 1, 0])\n",
        "assert (np.allclose(torch.nn.functional.softmax(torch.from_numpy(arr), dim=0).numpy(), softmax(arr)))\n",
        "arr = np.asarray([-750.0, 23, 9, 10, 230])\n",
        "assert (np.allclose(torch.nn.functional.softmax(torch.from_numpy(arr), dim=0).numpy(), softmax(arr)))\n",
        "arr = np.ones((4, ))\n",
        "assert (np.allclose(torch.nn.functional.softmax(torch.from_numpy(arr), dim=0).numpy(), softmax(arr)))\n",
        "arr = np.zeros((4, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_G-LNe1kW2"
      },
      "source": [
        "*Softmax temperature* is a concept that we'll be using later in this course.\n",
        "The *softmax temperature* is a hyper-parameter (positive number) which scales the input of the softmax function to modify the output probabilities.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "s(x, T)_i = \\frac{e^{x_i/T}}{\\sum_{j = 0}^{j = N} e^{x_j/T}}\n",
        "\\end{equation}\n",
        "\n",
        "Now modify your implementation of the softmax function such that it also takes as input the softmax temperature (a positive floating point number). If this parameter is not specified, it should default to 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYt1HAeE3Joc"
      },
      "source": [
        "Now let's visualise what is the effect of the softmax temperature.\n",
        "Given the input vector *x = [100, 2, -150, 75, 99, 3]* , plot the original vector and the softmax with temperatures $ T \\in \\{0.25, 0.75, 1, 1.5, 2, 5, 10, 20, 30\\} $.\n",
        "\n",
        "You can use a bar plot for this. The title for each plot should be the value of the softmax temperature. Also, make sure that for all the plots the range of the *y* axis is set to (0, 1).\n",
        "\n",
        "This code is found in _activations.py_ .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgonLQB60lH"
      },
      "source": [
        "# your code here\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.asarray([20, 30, -15, 45, 39, -10], dtype=np.float64)\n",
        "T = [0.25, 0.75, 1, 1.5, 2, 5, 10, 20, 30]\n",
        "nums = np.arange(len(x))\n",
        "\n",
        "for idx, temp in enumerate(T, start=1):\n",
        "  # TODO your code here\n",
        "  # plot the result of applying the softmax function\n",
        "  # with different temperatures on the array x\n",
        "  plt.subplot(3, 3, idx)\n",
        "  plt.bar(nums, softmax(x, temp))\n",
        "  plt.ylim(0, 1)\n",
        "  plt.title(f\"Temp {temp}\")\n",
        "  # end TODO your code here\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssG32V9U3dZt"
      },
      "source": [
        "Analyse the plots and answer the following questions:\n",
        "* What happens when we use a large number for the softmax temperature?\n",
        "  - The values after applying softmax tend to be more equally distributed, the values passed to the exponential function being closer to 0, and because of how the exponential function is more flat in the 0 area, all of the outputs will tend to 1 / n as we increase the temperature.\n",
        "* What happens when we use a small number (i.e. less than 1) for the softmax temperature?\n",
        "  - The values passed to the exponential function will increase, and because this function _explodes_ for big numbers, numpy will return infinity over infinity, which is nan and the output will probably break, in general, the effect of applying such temperature is the biggest value will shine even more.\n",
        "* In the context of image classification, the predicted class is determined by taking the *argmax* of the softmax function. Does the softmax temperature change in any way this prediction?\n",
        "  - No, the maximum of the softmax function is the same as the maximum before applying the softmax which whether temperature (dividing by a number wouldn't change the order of the numbers), just by how the computer perceives the numbers it can affect this, but mathematically it's impossible.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmzh0QjSMind"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We'll be using CIFAR-10 dataset.\n",
        "The dataset comprises 60000 colour images with a resolution of $32 \\times 32$, separated into 10 classes, with 6000 images per class. It is already split into train-test subsets, with 50000 training images and 10000 test images.\n",
        "\n",
        "You can download the data from this [link](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjTfrvpG3b8L"
      },
      "source": [
        "![ ! -d cifar-10-batches-py ] && wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz && tar -xvf cifar-10-python.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIyze33iqbWC"
      },
      "source": [
        "In the script *cifar10.py* you will complete the function ``load_cifar10``, which load the data from the archive you just downloaded.\n",
        "\n",
        "The images of this dataset are are stored in a numpy array, one image per row, in the following order:\n",
        "\n",
        "\"_The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image._\"\n",
        "\n",
        "Your task here is just to manipulate this array, such that each image has the shape (32, 32, 3) and uses RGB ordering."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.view('lab2/cifar10.py')"
      ],
      "metadata": {
        "id": "fxBvlO5OBCMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_My code_:\n",
        "\n",
        "```python\n",
        "def load_batch(filepath: str) -> (np.ndarray, np.ndarray):\n",
        "    \"\"\"\"\n",
        "    Loads a batch from the cifar-10 dataset.\n",
        "    A batch file contains a dictionary with the following elements:\n",
        "    data -- a 10000x3072 numpy array of uint8s.\n",
        "            Each row of the array stores a 32x32 colour image.\n",
        "            The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue.\n",
        "            The image is stored in row-major order, so that the first 32 entries of the array are the red channel values\n",
        "            of the first row of the image.\n",
        "    labels -- a list of 10000 numbers in the range 0-9.\n",
        "            The number at index i indicates the label of the ith image in the array data.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = pickle.load(f, encoding='latin1')\n",
        "        X = data['data']\n",
        "        y = data['labels']\n",
        "        # TODO your code here\n",
        "        # transform the X vector such that each element from the vector is 32x32 color image\n",
        "        # 0. first reshape the vector to (num_images, 3, 32, 32)\n",
        "        # then transpose it, such that the images are stored in (rows, cols, channels) order\n",
        "        X = np.reshape(X, (len(X), 3, 32, 32))\n",
        "        X = np.transpose(X, (0, 2, 3, 1))\n",
        "        # end TODO your code here\n",
        "        return X, np.asarray(y)\n",
        "```"
      ],
      "metadata": {
        "id": "jaFaimFokLco"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrwvGq7bhLXb"
      },
      "source": [
        "Now let's visualize some of the images from the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17wIc7-6hFst"
      },
      "source": [
        "  from lab2 import cifar10\n",
        "  cifar_root_dir = 'cifar-10-batches-py'\n",
        "  _, _, X_test, y_test = cifar10.load_ciaf10(cifar_root_dir)\n",
        "  indices = np.random.choice(len(X_test), 15)\n",
        "\n",
        "  display_images, display_labels = X_test[indices], y_test[indices]\n",
        "  for idx, (img, label) in enumerate(zip(display_images, display_labels)):\n",
        "      plt.subplot(3, 5, idx + 1)\n",
        "      plt.imshow(img)\n",
        "      plt.title(cifar10.LABELS[label])\n",
        "      plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOMOuotipyB6"
      },
      "source": [
        "## Image classification using a linear classifier in pytorch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Image classification refers to the problem of automatically assigning a label (a class) to an image $X_i$. As classification is a supervised learning problem, each image is assigned a ground truth label $y_i \\in \\{1, 2, ..., C\\}$.\n",
        "\n",
        "\n",
        "Today you will implement and train the simplest possible artificial neural network using pytorch.\n",
        "A linear classifier uses a function $f$ to map an input image $X_i \\in R^{H\\times W\\times 3}$ into a vector of C class scores  $\\hat y_i \\in R^C$:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "f(X_i) = X_i \\cdot W + b\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The learnable parameters of the classifier are the weight matrix $W$ and the bias vector $b$. Using the bias trick the bias term can be included in the weight matrix.\n",
        "\n",
        "\n",
        "During the training process, the values of the weight matrix are learned by minimizing a loss function (that penalizes the discrepancy between the predicted and the ground truth label).\n",
        "\n",
        "\n",
        "In your implementation, the bias trick is used so the bias value is already added in the weight matrix. In the file _train.py_:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "```\n",
        "\n",
        "\n",
        "## Implementation\n",
        "\n",
        "\n",
        "[Pytorch](https://pytorch.org/) is a powerful machine learning framework widely used for computer vision and natural language processing applications that we'll be using this semester.\n",
        "[Tensors](https://pytorch.org/docs/stable/tensors.html) are the main data abstraction from pytorch; similar to numpy arrays, they represent multi-dimensional arrays of a single data type.\n",
        "You can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements it contains (dtype) and what device the tensor lives on (CPU memory? CUDA memory?).\n",
        "\n",
        "\n",
        "It is quite easy to transform tensors to and from numpy arrays:\n",
        "- _from_numpy()_ function allows you to create a pytorch tensor from a numpy array.\n",
        "- _numpy()_ functions allows you to convert a tensor to a numpy() array.\n",
        "\n",
        "\n",
        "Also, you will use the function _item()_ to convert a tensor to a scalar python value.\n",
        "\n",
        "\n",
        "Another important feature of pytorch is the _autograd_ module that provides classes and functions for automatic differentiation of arbitrary scalar valued functions. This module is really easy to use, you only need to declare Tensor's for which gradients should be computed by setting the _requires_grad_ attribute of a tensor to True.\n",
        "As you know, most machine learning models are trained using the gradient descent algorithm. This is a first-order iterative optimization algorithm used to find the minimum of a differentiable function: the main idea is to take repeated steps in the opposite direction of the gradient  of the function at the current point (i.e. in the direction of steepest descent).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Softmax classification\n",
        "\n",
        "\n",
        "We'll follow an object oriented approach to solve this problem.\n",
        "All the code related to the softmax classifier will be implemented in the class *SoftmaxClassifier* (defined in the script _softmax.py_).\n",
        "\n",
        "\n",
        "The _SoftmaxClassifier_ class contains in the  attribute _self.W_ the weight matrix that will be learned during the training process.  \n",
        "\n",
        "\n",
        "The class comprises the following methods:\n",
        "\n",
        "\n",
        "| Method                               | Description |\n",
        "| ----------- | ----------- |\n",
        "| initialize()      | This function randomly initializes the weights of the linear classifier.    |\n",
        "| fit(X_train, y_train, **kwargs)      | This function will learn the weights of the model based on the training data samples (X_train) and their corresponding ground truth (y_train)       |\n",
        "|  predict(X)                | This function will return the classifier's prediction (the predicted class) for the data passed as parameter.        |\n",
        "|  predict_proba(X)                | This function will return the classifier's predictions for the data passed as parameters.        |\n",
        "|  save(path)   | This function will dump the weights of the classifier in the path specified as parameter.        |\n",
        "|  load(load)   | This function will load the classifier's weights from the path specified as parameter      |\n",
        "\n",
        "\n",
        "Feel free to add any additional helper methods if needed.\n",
        "\n",
        "\n",
        "#### Initialization and persistence\n",
        "* The constructor of this class takes as input the flattened size of the input image and the number of output classes, sets the corresponding class variables and calls the _init()_ method.\n",
        "* fill in the _initialize()_ method. You figure out the shape of the weight matrix based on the input shape and the number of classes. Initialize the weight matrix with small random variables.\n",
        "__The bias trick is already implemented__!\n",
        "* fill in the _save_ and _load_ functions. In these functions you should just dump and restore, respectively, the weight matrix to/from the specified file.\n",
        "\n",
        "\n",
        "#### Inference - the _predict_ and _predict\\_proba_ methods\n",
        "\n",
        "\n",
        "The inference is quite simple. You just need to compute the dot product between the input and the weight matrix.\n",
        "You will implement two inference methods:\n",
        "* _predict_ - this will just return the predicted class label. So you just need to compute the dot product and take the argmax of the result.\n",
        "* _predict\\_proba_ - this will return the class probabilities. So after computing the dot product, you also need to apply the softmax function on the result to normalize it to a probability distribution.\n",
        "\n",
        "\n",
        "If you implement this correctly, the _SoftmaxClassifier_ can be used to make predictions, but of course they are not very accurate as we haven't trained the model yet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_My code_:\n",
        "\n",
        "```python\n",
        "def initialize(self):\n",
        "    # TODO your code here\n",
        "    # initialize the weight matrix (remember the bias trick) with small random variables\n",
        "    # you might find torch.randn userful here *0.001\n",
        "    self.W = torch.randn(self.input_shape, self.num_classes) * 0.001\n",
        "    # don't forget to set call requires_grad_() on the weight matrix,\n",
        "    # as we will be be taking its gradients during the learning process\n",
        "    self.W.requires_grad_()\n",
        "```\n",
        "```python\n",
        "def save(self, path: str) -> bool:\n",
        "    # save the input shape, the number of classes and the weight matrix to a file\n",
        "    # you might find torch useful for this\n",
        "    # TODO your code here\n",
        "    torch.save({\n",
        "        \"input_shape\": self.input_shape,\n",
        "        \"num_classes\": self.num_classes,\n",
        "        \"W\": self.W,\n",
        "    }, path)\n",
        "    return True\n",
        "```\n",
        "```python\n",
        "def load(self, path: str) -> bool:\n",
        "    # TODO your code here\n",
        "    # load the input shape, the number of classes and the weight matrix from a file\n",
        "    # you might find torch.load useful here\n",
        "    checkpoint = torch.load(path)\n",
        "    self.W = checkpoint[\"W\"]\n",
        "    # don't forget to set the input_shape and num_classes fields\n",
        "    self.num_classes = checkpoint[\"num_classes\"]\n",
        "    self.input_shape = checkpoint[\"input_shape\"]\n",
        "    return True\n",
        "```\n",
        "```python\n",
        "def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO your code here\n",
        "    # 0. compute the dot product between the input X and the weight matrix\n",
        "    scores = X @ self.W\n",
        "    # 1. compute the prediction by taking the argmax of the class scores\n",
        "    # you might find torch.argmax useful here.\n",
        "    # think about on what dimension (dim parameter) you should apply this operation\n",
        "    label = torch.argmax(scores, dim=1)\n",
        "    return label\n",
        "```\n",
        "```python\n",
        "def predict_proba(self, X: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO your code here\n",
        "    # 0. compute the dot product between the input X and the weight matrix\n",
        "    # you can use @ for this operation\n",
        "    scores = X @ self.W\n",
        "    # remember about the bias trick!\n",
        "    # 1. apply the softmax function on the scores, see torch.nn.functional.softmax\n",
        "    # think about on what dimension (dim parameter) you should apply this operation\n",
        "    scores = torch.nn.functional.softmax(scores, dim=1)\n",
        "    # 2. returned the normalized scores\n",
        "    return scores\n",
        "```"
      ],
      "metadata": {
        "id": "V-SKqKmIrmF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.view('lab2/softmax.py')"
      ],
      "metadata": {
        "id": "4M41lMkcFiOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lab2.softmax import SoftmaxClassifier\n",
        "num_pixels = np.prod(X_test[0].shape)\n",
        "cls = SoftmaxClassifier(num_pixels+1, len(cifar10.LABELS))\n",
        "\n",
        "test_example = torch.from_numpy(np.append(X_test[0].flatten(), 1.0)).float()\n",
        "test_example = test_example[None, :]\n",
        "print('predicted class ', cifar10.LABELS[cls.predict(test_example)], cls.predict(test_example))\n",
        "print('probas: ', cls.predict_proba(test_example).detach().numpy())"
      ],
      "metadata": {
        "id": "8oYoXZfva9l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training - the _fit_ method\n",
        "\n",
        "The training process is implemented in the function _fit_ : here we are interested in finding the values for the weight matrix such that the classifier gives accurate predictions.\n",
        "\n",
        "The loss function for the softmax classifier is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "L_i = - \\sum_{i=1}^{C} log(\\hat{y\\_pred}_i)\\cdot y_i.\n",
        "\\end{equation}\n",
        "\n",
        ", where $\\hat{y_p}_i$ are the probabilities returned by the classifier for the ith sample and $y_i$ is the corresponding ground truth (as one hot encoding).\n",
        "\n",
        "So in the ``cross_entropy`` function you just take the negative of the logarithm of the predicted probability for the ground truth class.\n",
        "\n",
        "In the case of the softmax classifier $\\hat{y\\_pred}_i$ is always a value in the interval [0, 1] (we use the softmax function to normalize the logits into a probability distribution).\n",
        "When the probability of the ground truth class is small (i.e. close to 0), the loss will be very high (theoretically, infinite) [log(0) = -inf]. On the other hand, when the probability of the correct class will be high (i.e. close to 1), then the loss will be close to 0 [log(1) = 0].\n",
        "\n",
        "For numerical staibility instead of using softmax, we'll use the log softmax.\n",
        "\n",
        "In the function `log_softmax` implement the following:\n",
        "\\begin{equation}\n",
        "log\\_softmax(x_i) =  log(\\frac{e^{x_i}}{\\sum_{j}e^{x_j}}) = x_i - log(\\sum_j e^x_j)\n",
        "\\end{equation}\n",
        "\n",
        "Then, in the ``fit`` method:\n",
        "\\begin{equation}\n",
        "y\\_pred_i = f(x_i)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y\\_pred_i} =  log\\_softmax(y\\_pred_i)\n",
        "\\end{equation},\n",
        "\n",
        "and take the cross entropy loss between $\\hat{y\\_pred_i}$ and $y_i$ to compute the data loss.\n",
        "\n",
        "For this we need two steps:\n",
        "1. we need to define a loss function that quantifies our\n",
        "how well is the classifier doing on the training data. The loss function that we will use is the **cross entropy loss**.\n",
        "Remember from the lecture that, in order to avoid overfitting, a regularization term is added to the loss function.\n",
        "You will implement the $L_2$ regularization, in which you also add the  sum of squares of all of the feature weights to the loss.\n",
        "\n",
        "\n",
        "2. we need an algorithm to efficiently find the\n",
        "parameters that minimize the loss function.\n",
        "\n",
        "Pytorch provides a powerful feature, _autograd_: it allows for the rapid and easy computation of multiple partial derivatives (also referred to as gradients) over a complex computation. This operation is central to backpropagation-based neural network learning.\n",
        "\n",
        "You can follow [this](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html) post for more details about how autograd works.\n",
        "We'll cover autograd in more detail in the next labs.\n",
        "\n",
        "For now, we'll be using it to make your life easier when implementing gradient descent to train the softmax classifier. In gradient descent, the idea is to take repeated steps in the opposite direction of the gradient of the loss function at the current point. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, and iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
        "\n",
        "To signal autograd that it should collect the gradients of a tensor, you need to pass the parameter _requires_grad=True_ when creating it or call the method _requires_grad_()_ on a tensor.\n",
        "\n",
        "To __start the backpropagation__ with autograd, you just need to call the _backward()_ function on the loss tensor (a scalar).  \n",
        "Autograd calculates and stores the gradients for each model parameter in the tensors' _grad_ field.\n",
        "\n",
        "Let's see how a custom training loop looks in pytorch:\n",
        "\n",
        "```python\n",
        "lr = 0.05  # learning rate: what is the size of the step that we will take in the opposite direction of the gradient\n",
        "epochs = 32  # how many epochs (passes through the traing data) we'll peform\n",
        "bs = 32 # batch size:  we'll update the parameters after seeing several examples (a batch of data)\n",
        "\n",
        "for epoch in range(epochs):    \n",
        "    for ii in range((X_train.shape[0] - 1) // bs + 1):  # in batches of size bs\n",
        "        start_idx = ii * bs  # we are ii batches in, each of size bs\n",
        "        end_idx = start_idx + bs  # get bs examples\n",
        "\n",
        "        # get the training training examples xb, and their coresponding annotations\n",
        "        xb = X_train[start_idx:end_idx]\n",
        "        yb = y_train[start_idx:end_idx]\n",
        "\n",
        "        # apply the model on the training examples\n",
        "        pred = predict_proba(xb)\n",
        "\n",
        "        # compute the loss function\n",
        "        loss = cross_entropy_loss(pred, yb)\n",
        "\n",
        "        # start backpropagation: calculate the gradients with a backwards pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters\n",
        "        with torch.no_grad():  # we don't want to track gradients\n",
        "            # take a step in the negative direction of the gradient, the learning rate defines the step size\n",
        "            weights -= weights.grad * lr\n",
        "\n",
        "            # ATTENTION: you need to explictly set the gradients to 0 (let pytorch know that you are done with them).\n",
        "            weights.grad.zero_()\n",
        "            \n",
        "``\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfVor2hAa6nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_My code_:\n",
        "\n",
        "#### softmax.py\n",
        "```python\n",
        "def cross_entropy_loss(self, y_pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO your code here\n",
        "    loss = -torch.log(y_pred[range(len(y)), y] + 1e-10)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "def log_softmax(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return x - torch.log(torch.sum(torch.exp(x), dim=0))\n",
        "\n",
        "def l2_regularizer(self) -> torch.Tensor:\n",
        "    return torch.sum(self.W * self.W)\n",
        "\n",
        "def fit(self, X_train: torch.Tensor, y_train: torch.Tensor,\n",
        "        **kwargs) -> dict:\n",
        "    history = []\n",
        "\n",
        "    bs = kwargs['bs'] if 'bs' in kwargs else 128\n",
        "    reg_strength = kwargs['reg_strength'] if 'reg_strength' in kwargs else 1e3\n",
        "    epochs = kwargs['epochs'] if 'epochs' in kwargs else 100\n",
        "    lr = kwargs['lr'] if 'lr' in kwargs else 1e-3\n",
        "    print('hyperparameters: lr {:.4f}, reg {:.4f}, epochs {:.2f}'.format(lr, reg_strength, epochs))\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for ii in range((X_train.shape[0] - 1) // bs + 1):  # in batches of size bs\n",
        "            # TODO your code here\n",
        "            start_idx = ii * bs  # we are ii batches in, each of size bs\n",
        "            end_idx = start_idx + bs  # get bs examples\n",
        "\n",
        "            # get the training training examples xb, and their coresponding annotations\n",
        "            xb = X_train[start_idx:end_idx]\n",
        "            yb = y_train[start_idx:end_idx]\n",
        "\n",
        "            # apply the linear layer on the training examples from the current batch\n",
        "            pred = self.predict_proba(xb)\n",
        "            # pred = self.log_softmax(pred)\n",
        "            # compute the loss function\n",
        "            # also add the L2 regularization loss (the sum of the squared weights)\n",
        "            loss = self.cross_entropy_loss(pred, yb) + reg_strength * self.l2_regularizer()\n",
        "            history.append(loss.detach().numpy())\n",
        "            # start backpropagation: calculate the gradients with a backwards pass\n",
        "            loss.backward()\n",
        "\n",
        "            # update the parameters\n",
        "            with torch.no_grad():  # we don't want to track gradients\n",
        "                # take a step in the negative direction of the gradient, the learning rate defines the step size\n",
        "                self.W -= self.W.grad * lr\n",
        "\n",
        "                # ATTENTION: you need to explictly set the gradients to 0 (let pytorch know that you are done with them).\n",
        "                self.W.grad.zero_()\n",
        "        if epoch % 5 == 0:\n",
        "          print(f\"Epoch: {epoch}/{epochs}\\tLoss: {np.mean(history[(epoch - 1) * ((X_train.shape[0] - 1) // bs + 1):])}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def get_weights(self, img_shape) -> np.ndarray:\n",
        "    # TODO your code here\n",
        "    W = self.W.detach().numpy()\n",
        "    # 0. ignore the bias term\n",
        "    W = W[:-1]\n",
        "    # 1. reshape the weights to (*image_shape, num_classes)\n",
        "    W = np.reshape(W, (*img_shape, self.num_classes))\n",
        "    # you might find the transpose function useful here\n",
        "    W = np.transpose(W, (3, 0, 1, 2))\n",
        "    return W\n",
        "```\n",
        "#### metrics.py\n",
        "```python\n",
        "def get_num_classes(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    return max(np.max(y_pred), np.max(y_true)) + 1\n",
        "\n",
        "\n",
        "def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, num_classes = None) -> np.ndarray:\n",
        "    \"\"\"\"\n",
        "    Computes the confusion matrix from labels (y_true) and predictions (y_pred).\n",
        "    The matrix columns represent the prediction labels and the rows represent the ground truth labels.\n",
        "    The confusion matrix is always a 2-D array of shape `[num_classes, num_classes]`,\n",
        "    where `num_classes` is the number of valid labels for a given classification task.\n",
        "    The arguments y_true and y_pred must have the same shapes in order for this function to work\n",
        "\n",
        "    num_classes represents the number of classes for the classification problem. If this is not provided,\n",
        "    it will be computed from both y_true and y_pred\n",
        "    \"\"\"\n",
        "    # TODO your code here - compute the confusion matrix\n",
        "    # even here try to use vectorization, so NO for loops\n",
        "\n",
        "    # 0. if the number of classes is not provided, compute it based on the y_true and y_pred arrays\n",
        "    if num_classes == None:\n",
        "        num_classes = get_num_classes(y_true, y_pred)\n",
        "    # 1. create a confusion matrix of shape (num_classes, num_classes) and initialize it to 0\n",
        "    conf_mat = np.zeros((num_classes, num_classes), dtype=np.int16)\n",
        "    # 2. use argmax to get the maximal prediction for each sample\n",
        "    np.add.at(conf_mat, (y_true, y_pred), 1)\n",
        "    # hint: you might find np.add.at useful: https://numpy.org/doc/stable/reference/generated/numpy.ufunc.at.html\n",
        "\n",
        "    # end TODO your code here\n",
        "    return conf_mat\n",
        "\n",
        "\n",
        "def precision_score(y_true: np.ndarray, y_pred: np.ndarray, num_classes=None) -> float:\n",
        "    \"\"\"\"\n",
        "    Computes the precision score.\n",
        "    For binary classification, the precision score is defined as the ratio tp / (tp + fp)\n",
        "    where tp is the number of true positives and fp the number of false positives.\n",
        "\n",
        "    For multiclass classification, the precision and recall scores are obtained by summing over the rows / columns\n",
        "    of the confusion matrix.\n",
        "\n",
        "    num_classes represents the number of classes for the classification problem. If this is not provided,\n",
        "    it will be computed from both y_true and y_pred\n",
        "    \"\"\"\n",
        "    # TODO your code here\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, num_classes)\n",
        "    precision = np.diag(conf_matrix) / np.maximum(1, np.sum(conf_matrix, axis=0))\n",
        "    # end TODO your code here\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall_score(y_true: np.ndarray, y_pred: np.ndarray, num_classes=None)  -> float:\n",
        "    \"\"\"\"\n",
        "    Computes the recall score.\n",
        "    For binary classification, the recall score is defined as the ratio tp / (tp + fn)\n",
        "    where tp is the number of true positives and fn the number of false negatives\n",
        "\n",
        "    For multiclass classification, the precision and recall scores are obtained by summing over the rows / columns\n",
        "    of the confusion matrix.\n",
        "\n",
        "    num_classes represents the number of classes for the classification problem. If this is not provided,\n",
        "    it will be computed from both y_true and y_pred\n",
        "    \"\"\"\n",
        "    # TODO your code here\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, num_classes)\n",
        "    recall = np.diag(conf_matrix) / np.maximum(1, np.sum(conf_matrix, axis=1))\n",
        "    # end TODO your code here\n",
        "    return recall\n",
        "\n",
        "\n",
        "def accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    # TODO your code here\n",
        "    # remember, use vectorization, so no for loops\n",
        "    # hint: you might find np.trace useful here https://numpy.org/doc/stable/reference/generated/numpy.trace.html\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    acc_score = np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
        "    # end TODO your code here\n",
        "    return acc_score\n",
        "```"
      ],
      "metadata": {
        "id": "ErSJda3cx366"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the fit method.\n",
        "\n",
        "## Putting it all together\n",
        "\n",
        "\n",
        "By now you have implemented all the blocks needed to train a softmax classifier. Now it is time to train the classifier using different hyperparameters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Try different values for the hyper-parameters of the classifier (the learning rate $\\lambda$ and the regularization strength $\\rho$).\n",
        "The _train.py_ script contains the code that you will use for training and comparing different settings for  $\\lambda$ and $\\rho$.\n",
        "\n",
        "\n",
        "Finally, after you trained a classifier display the learned weights for each of the classes in the cifar10 dataset. Intuitively, each row in the weight matrix would be considered as \"template\" for that class.\n"
      ],
      "metadata": {
        "id": "IUcvAZ7BJPGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.view('lab2/train.py')"
      ],
      "metadata": {
        "id": "xNyesXQ5DSdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import lab2.cifar10\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from lab2.softmax import SoftmaxClassifier\n",
        "from lab2.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "\n",
        "cifar_root_dir = 'cifar-10-batches-py'\n",
        "\n",
        "# load cifar10 dataset\n",
        "X_train, y_train, X_test, y_test = cifar10.load_ciaf10(cifar_root_dir)\n",
        "\n",
        "# convert the training and test data to floating point\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "# Reshape the training data such that we have one image per row\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "# pre-processing: subtract mean image\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "X_train -= mean_image\n",
        "X_test -= mean_image\n",
        "\n",
        "# Bias trick - add 1 to each training example\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "\n",
        "# convert everything to tensors\n",
        "X_train, y_train, X_test, y_test = map(\n",
        "    torch.tensor, (X_train, y_train, X_test, y_test)\n",
        ")\n",
        "\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()\n",
        "\n",
        "\n",
        "if not os.path.exists('train'):\n",
        "    os.mkdir('train')\n",
        "\n",
        "best_acc = -1\n",
        "best_cls_path = ''\n",
        "\n",
        "\n",
        "input_size_flattened = reduce((lambda a, b: a * b), X_train[0].shape)\n",
        "\n",
        "# the batch size\n",
        "batch_size = 200\n",
        "# number of training steps per training process\n",
        "train_epochs = 200\n",
        "\n",
        "\n",
        "lr = 0.007 # change the value - hyperparameter tuning\n",
        "reg_strength = 0.001 # change the value - hyperparameter tuning\n",
        "\n",
        "cls = SoftmaxClassifier(input_shape=input_size_flattened, num_classes=cifar10.NUM_CLASSES)\n",
        "history = cls.fit(X_train, y_train, lr=lr, reg_strength=reg_strength,\n",
        "        epochs=train_epochs, bs=batch_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_train_pred = cls.predict(X_train)\n",
        "  y_val_pred = cls.predict(X_test)\n",
        "\n",
        "train_acc = torch.mean((y_train == y_train_pred).float())\n",
        "\n",
        "test_acc = torch.mean((y_test == y_val_pred).float())\n",
        "sys.stdout.write('\\rlr {:.4f}, reg_strength{:.2f}, test_acc {:.2f}; train_acc {:.2f}'.format(lr, reg_strength, test_acc, train_acc))\n",
        "cls_path = os.path.join('train', 'softmax_lr{:.4f}_reg{:.4f}-test{:.2f}.npy'.format(lr, reg_strength, test_acc))\n",
        "cls.save(cls_path)\n",
        "\n",
        "\n",
        "plt.plot(history)\n",
        "plt.show()\n",
        "\n",
        "best_softmax = cls\n",
        "\n",
        "\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "# now let's display the weights for the best model\n",
        "weights = best_softmax.get_weights((32, 32, 3))\n",
        "\n",
        "w_min = np.amin(weights)\n",
        "w_max = np.amax(weights)\n",
        "\n",
        "for idx in range(0, cifar10.NUM_CLASSES):\n",
        "    plt.subplot(2, 5, idx + 1)\n",
        "    # normalize the weights\n",
        "    template = 255.0 * (weights[idx, :, :, :].squeeze() - w_min) / (w_max - w_min)\n",
        "    template = template.astype(np.uint8)\n",
        "    plt.imshow(template)\n",
        "    plt.title(cifar10.LABELS[idx])\n",
        "\n",
        "plt.show()\n",
        "# TODO your code here\n",
        "# use the metrics module to compute the precision, recall and confusion matrix for the best classifier\n",
        "y_true = y_test.numpy()\n",
        "y_pred = y_val_pred.numpy()\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred))\n",
        "print(\"Confusion matrix\\n\", confusion_matrix(y_true, y_pred))\n",
        "# end TODO your code here\n"
      ],
      "metadata": {
        "id": "iAnc0ZrrDtSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5U_lzCrht3-"
      },
      "source": [
        "## Refactoring using torch components\n",
        "\n",
        "\n",
        "Luckily pytorch provides implementation for most of the layers used in modern machine learning, as well as loss functions, metrics, and optimization algorithms.\n",
        "\n",
        "\n",
        "To define a model, you must extend the torch.nn.Module class which is the base class for all the neural network modules. In the constructor, you define the layers (and their properties) that comprise your module. The line\n",
        "\n",
        "\n",
        "Another important function that you need to override is the forward() function in which you specify computation performed at every call (i.e. how are layers chained and how does the data flow over the computational graph). In other words, this defines the forward pass through your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Cifar10Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(32 * 32 * 3, 10) # TODO create a linear layer nn.Linear\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "        return x  # apply the linear layer on the input data"
      ],
      "metadata": {
        "id": "V17YiNcCE8Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `nn.Linear` layer is a \"child\"\n",
        "of the `Cifar10Classifier`,\n",
        "and the weight matrix is abstracted (you don't have direct access to it).\n"
      ],
      "metadata": {
        "id": "MFuCM3tNGywS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Cifar10Classifier()\n",
        "print(*list(model.children()))"
      ],
      "metadata": {
        "id": "3R1ujZDsHP6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But you can still access the parameters of the model (the bias trick is not used in this case):"
      ],
      "metadata": {
        "id": "Dq3OduPsHenw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(*list(model.parameters()), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "2Q_iQdzAHpxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch also provides several loss functions that you can use in the `torch.nn.functional` model. Let's drop the log_softmax and cross_entropy function that you wrote and use `torch.nn.functional.cross_entropy`[function](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html).\n",
        "Pay attention: you don't need to apply the log-softmax on the prediction of the model as the function operates directly on  unnormalized logits."
      ],
      "metadata": {
        "id": "lES_WyDQIu0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = torch.nn.functional.cross_entropy"
      ],
      "metadata": {
        "id": "36I1IPCkbUYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimization process (applying gradients to find the parameters and resetting those gradients to zero) can be handled more elegantly using predefined pytorch classes and operations.\n",
        "\n",
        "\n",
        "You just need a subclass of the torch.optim.Optimizer, and it will automatically update the parameters of our model. In this example, we'll be using the Adam optimizer (which will be covered in detail in the lectures).\n"
      ],
      "metadata": {
        "id": "Kxk4VqvJH2Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
        "    return optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "bA4kd000IQd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's reload the data:"
      ],
      "metadata": {
        "id": "kawSO_qgdtFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lab2 import cifar10\n",
        "cifar_root_dir = 'cifar-10-batches-py'\n",
        "\n",
        "# load cifar10 dataset\n",
        "X_train, y_train, X_test, y_test = cifar10.load_ciaf10(cifar_root_dir)\n",
        "\n",
        "# convert the training and test data to floating point\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "# Reshape the training data such that we have one image per row\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "# pre-processing: subtract mean image\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "X_train -= mean_image\n",
        "X_test -= mean_image\n",
        "\n",
        "# convert everything to tensors\n",
        "X_train, y_train, X_test, y_test = map(\n",
        "    torch.tensor, (X_train, y_train, X_test, y_test)\n",
        ")\n",
        "\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()"
      ],
      "metadata": {
        "id": "6fEe8ipmdxTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the training loop will be much simpler:"
      ],
      "metadata": {
        "id": "9L04-uX9IeKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "model = Cifar10Classifier()\n",
        "opt = configure_optimizer(model)\n",
        "\n",
        "\n",
        "print(\"before training:\", loss_func(model(X_test), y_test), sep=\"\\n\\t\")\n",
        "epochs = 50\n",
        "bs = 64\n",
        "num_train = X_train.shape[0]\n",
        "\n",
        "epochs = tqdm.tqdm(range(epochs), desc=\"Epochs\")\n",
        "\n",
        "for epoch in epochs:\n",
        "    for ii in range((num_train - 1) // bs + 1):\n",
        "        start_idx = ii * bs\n",
        "        end_idx = start_idx + bs\n",
        "        xb = X_train[start_idx:end_idx]\n",
        "        yb = y_train[start_idx:end_idx]\n",
        "        pred = model(xb) # call the forward function\n",
        "        loss = loss_func(pred, yb) # apply the loss function\n",
        "\n",
        "        loss.backward() # start the backpropagation and compute the gradients\n",
        "        opt.step() # apply the parameter update\n",
        "        opt.zero_grad() # zero out the gradients\n",
        "\n",
        "y_train_pred = torch.argmax(model(X_train), dim=1)\n",
        "train_acc = torch.sum(y_train == y_train_pred) / y_train.shape[0]\n",
        "y_val_pred = torch.argmax(model(X_test), dim=1)\n",
        "test_acc = torch.sum(y_test == y_val_pred) / y_test.shape[0]\n",
        "print('Train acc ', train_acc.detach().numpy(), ' test acc ', test_acc.detach().numpy())\n",
        "\n",
        "print(\"after training:\", loss_func(model(X_test), y_test), sep=\"\\n\\t\")"
      ],
      "metadata": {
        "id": "AK4-_AmiIg0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwVLNAq7JPvU"
      },
      "source": [
        "## Classifier evaluation\n",
        "\n",
        "After the training process, you want to evaluate the model on the test set, such that you can get an idea on how well your model will perform on unseen data.\n",
        "Ideally the classes in the test set should be balanced (i.e. you should have the same number of samples for each one of the classes).\n",
        "\n",
        "### Confusion matrix and classification metrics\n",
        "\n",
        "The confusion matrix can be considered the foundation stone for evaluating a classifier. As the name states, it's a simple way of visualising whether/how the model is confusing the classes.\n",
        "\n",
        "Each row of the confusion matrix represents the instances of the ground truth class, while each column represents the instances of the predicted class.\n",
        "\n",
        "<img src=\"https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg\"/>\n",
        "\n",
        "Based on the confusion matrix, you can compute different classification metrics:\n",
        "* *accuracy*: this is simply the ratio between the correctly classified samples (either positive or negative) and the total number of samples;\n",
        "* _precisi**on**_ : this metric measures the ability of the classifier to capture **only** relevant samples;\n",
        "* _rec**all**_ : this metric measures the ability of the classifier to spot **all** positive samples.\n",
        "\n",
        "As you may have noticed, it is not possible to maximize precision and recall at the same time, as one comes at the cost of another.\n",
        "Therefore, the $F_1$ score -- the harmonic mean between precision and recall -- was defined to combine these two metrics into a single numerical value.\n",
        "\n",
        "\\begin{equation}\n",
        "F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n",
        "\\end{equation}\n",
        "\n",
        "In the file *metrics.py* you should fill in the code for computing the accuracy, precision, recall and f1-score of your classifier.\n",
        "\n",
        "One challenge when implementing these metrics is that you are not allowed to use any repetitive loops (only numpy vectorization).\n",
        "\n",
        "Of course, pytorch provides functions for all these metrics, but for didactical puropose and to gain a deeper understanding of nd-arrays and tenors you should implement them from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0y6fkhMEcUS"
      },
      "source": [
        "Compute the metrics on the best classifier you obtained so far."
      ]
    }
  ]
}